{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17011885 박세정 과제 2) 동물 분류문제-최근",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwwLGsaCadDtSDmROQuQBS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sejeong-park/2020AI/blob/master/17011885_%EB%B0%95%EC%84%B8%EC%A0%95_%EA%B3%BC%EC%A0%9C_2)_%EB%8F%99%EB%AC%BC_%EB%B6%84%EB%A5%98%EB%AC%B8%EC%A0%9C_%EC%B5%9C%EA%B7%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLcslkWqCWeA",
        "colab_type": "code",
        "outputId": "14ada417-d255-4e2e-9e6a-643cfe32de68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "! curl 'https://raw.githubusercontent.com/sejongresearch/2020.Spring.AI/master/HW/zoo_class.csv' -o 'zoo_class.csv'\n",
        "print('Done')\n",
        "\n",
        "xy = np.loadtxt('zoo_class.csv', delimiter=',', dtype=np.float32)\n",
        "\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]].squeeze()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3535  100  3535    0     0  26578      0 --:--:-- --:--:-- --:--:-- 26780\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID9NVUZD04JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xy=np.loadtxt('zoo_class.csv',delimiter=',',dtype=np.float32)\n",
        "\n",
        "x_data=xy[:,0:-1]\n",
        "y_data=xy[:,[-1]].squeeze()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYHleeUu5UAC",
        "colab_type": "code",
        "outputId": "dfc1ce62-d5fa-4c13-efbc-238766e282e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "x_train=torch.FloatTensor(x_data)\n",
        "y_train=torch.LongTensor(y_data)\n",
        "\n",
        "print(x_train[:5])\n",
        "print(y_train[:5])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 4., 0., 0., 1.],\n",
            "        [1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 4., 1., 0., 1.],\n",
            "        [0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 4., 0., 0., 1.],\n",
            "        [1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 4., 1., 0., 1.]])\n",
            "tensor([0, 0, 3, 0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud4HHflM7fJx",
        "colab_type": "code",
        "outputId": "2ea58927-bb79-43e9-fa7e-bdbdb0c20369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "#모델학습\n",
        "nb_class=7\n",
        "nb_data=len(y_train)\n",
        "y_one_hot=torch.zeros(nb_data,nb_class)\n",
        "y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQzBnH0k7NxU",
        "colab_type": "code",
        "outputId": "beb54062-9244-4fba-e2f6-d567b8031a74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "#모델 초기화\n",
        "#feture 4개, 3개의 클래스\n",
        "nb_class=7\n",
        "nb_data=len(y_train)\n",
        "\n",
        "W=torch.zeros((16,nb_class),requires_grad=True)\n",
        "b=torch.zeros(nb_class,requires_grad=True)\n",
        "\n",
        "#optimizer설정\n",
        "optimizer=optim.SGD([W,b],lr=0.1)\n",
        "\n",
        "\n",
        "nb_epochs=10000;\n",
        "for epoch in range(nb_epochs+1):\n",
        "  #Cost계산\n",
        "  hypothesis=F.softmax(x_train.matmul(W)+b,dim=1)\n",
        "\n",
        "  #Cost 표현법 1번 예시\n",
        "  y_one_hot=torch.zeros(nb_data,nb_class)\n",
        "  y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
        "  cost=(y_one_hot*-torch.log(F.softmax(hypothesis,dim=1))).sum(dim=1).mean()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%100==0:\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch,nb_epochs,cost.item()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 7600/10000 Cost: 1.299701\n",
            "Epoch 7700/10000 Cost: 1.299420\n",
            "Epoch 7800/10000 Cost: 1.299147\n",
            "Epoch 7900/10000 Cost: 1.298883\n",
            "Epoch 8000/10000 Cost: 1.298627\n",
            "Epoch 8100/10000 Cost: 1.298378\n",
            "Epoch 8200/10000 Cost: 1.298135\n",
            "Epoch 8300/10000 Cost: 1.297898\n",
            "Epoch 8400/10000 Cost: 1.297666\n",
            "Epoch 8500/10000 Cost: 1.297438\n",
            "Epoch 8600/10000 Cost: 1.297214\n",
            "Epoch 8700/10000 Cost: 1.296993\n",
            "Epoch 8800/10000 Cost: 1.296774\n",
            "Epoch 8900/10000 Cost: 1.296558\n",
            "Epoch 9000/10000 Cost: 1.296342\n",
            "Epoch 9100/10000 Cost: 1.296127\n",
            "Epoch 9200/10000 Cost: 1.295912\n",
            "Epoch 9300/10000 Cost: 1.295697\n",
            "Epoch 9400/10000 Cost: 1.295481\n",
            "Epoch 9500/10000 Cost: 1.295265\n",
            "Epoch 9600/10000 Cost: 1.295047\n",
            "Epoch 9700/10000 Cost: 1.294827\n",
            "Epoch 9800/10000 Cost: 1.294606\n",
            "Epoch 9900/10000 Cost: 1.294384\n",
            "Epoch 10000/10000 Cost: 1.294161\n",
            "Epoch    0/10000 Cost: 1.945910\n",
            "Epoch  100/10000 Cost: 1.604912\n",
            "Epoch  200/10000 Cost: 1.493645\n",
            "Epoch  300/10000 Cost: 1.461950\n",
            "Epoch  400/10000 Cost: 1.448962\n",
            "Epoch  500/10000 Cost: 1.441627\n",
            "Epoch  600/10000 Cost: 1.436795\n",
            "Epoch  700/10000 Cost: 1.433280\n",
            "Epoch  800/10000 Cost: 1.430519\n",
            "Epoch  900/10000 Cost: 1.428198\n",
            "Epoch 1000/10000 Cost: 1.426118\n",
            "Epoch 1100/10000 Cost: 1.424141\n",
            "Epoch 1200/10000 Cost: 1.422154\n",
            "Epoch 1300/10000 Cost: 1.420064\n",
            "Epoch 1400/10000 Cost: 1.417789\n",
            "Epoch 1500/10000 Cost: 1.415266\n",
            "Epoch 1600/10000 Cost: 1.412451\n",
            "Epoch 1700/10000 Cost: 1.409229\n",
            "Epoch 1800/10000 Cost: 1.404665\n",
            "Epoch 1900/10000 Cost: 1.379627\n",
            "Epoch 2000/10000 Cost: 1.360749\n",
            "Epoch 2100/10000 Cost: 1.356140\n",
            "Epoch 2200/10000 Cost: 1.353121\n",
            "Epoch 2300/10000 Cost: 1.350830\n",
            "Epoch 2400/10000 Cost: 1.348962\n",
            "Epoch 2500/10000 Cost: 1.347376\n",
            "Epoch 2600/10000 Cost: 1.345995\n",
            "Epoch 2700/10000 Cost: 1.344774\n",
            "Epoch 2800/10000 Cost: 1.343681\n",
            "Epoch 2900/10000 Cost: 1.342692\n",
            "Epoch 3000/10000 Cost: 1.341790\n",
            "Epoch 3100/10000 Cost: 1.340958\n",
            "Epoch 3200/10000 Cost: 1.340183\n",
            "Epoch 3300/10000 Cost: 1.339454\n",
            "Epoch 3400/10000 Cost: 1.338758\n",
            "Epoch 3500/10000 Cost: 1.338083\n",
            "Epoch 3600/10000 Cost: 1.337415\n",
            "Epoch 3700/10000 Cost: 1.336739\n",
            "Epoch 3800/10000 Cost: 1.336034\n",
            "Epoch 3900/10000 Cost: 1.335270\n",
            "Epoch 4000/10000 Cost: 1.334406\n",
            "Epoch 4100/10000 Cost: 1.333384\n",
            "Epoch 4200/10000 Cost: 1.332119\n",
            "Epoch 4300/10000 Cost: 1.330515\n",
            "Epoch 4400/10000 Cost: 1.328519\n",
            "Epoch 4500/10000 Cost: 1.326241\n",
            "Epoch 4600/10000 Cost: 1.323979\n",
            "Epoch 4700/10000 Cost: 1.321999\n",
            "Epoch 4800/10000 Cost: 1.320349\n",
            "Epoch 4900/10000 Cost: 1.318938\n",
            "Epoch 5000/10000 Cost: 1.317661\n",
            "Epoch 5100/10000 Cost: 1.316431\n",
            "Epoch 5200/10000 Cost: 1.315176\n",
            "Epoch 5300/10000 Cost: 1.313837\n",
            "Epoch 5400/10000 Cost: 1.312421\n",
            "Epoch 5500/10000 Cost: 1.311060\n",
            "Epoch 5600/10000 Cost: 1.309886\n",
            "Epoch 5700/10000 Cost: 1.308899\n",
            "Epoch 5800/10000 Cost: 1.308041\n",
            "Epoch 5900/10000 Cost: 1.307273\n",
            "Epoch 6000/10000 Cost: 1.306571\n",
            "Epoch 6100/10000 Cost: 1.305923\n",
            "Epoch 6200/10000 Cost: 1.305323\n",
            "Epoch 6300/10000 Cost: 1.304762\n",
            "Epoch 6400/10000 Cost: 1.304236\n",
            "Epoch 6500/10000 Cost: 1.303742\n",
            "Epoch 6600/10000 Cost: 1.303277\n",
            "Epoch 6700/10000 Cost: 1.302836\n",
            "Epoch 6800/10000 Cost: 1.302419\n",
            "Epoch 6900/10000 Cost: 1.302023\n",
            "Epoch 7000/10000 Cost: 1.301646\n",
            "Epoch 7100/10000 Cost: 1.301286\n",
            "Epoch 7200/10000 Cost: 1.300942\n",
            "Epoch 7300/10000 Cost: 1.300613\n",
            "Epoch 7400/10000 Cost: 1.300297\n",
            "Epoch 7500/10000 Cost: 1.299994\n",
            "Epoch 7600/10000 Cost: 1.299701\n",
            "Epoch 7700/10000 Cost: 1.299420\n",
            "Epoch 7800/10000 Cost: 1.299147\n",
            "Epoch 7900/10000 Cost: 1.298883\n",
            "Epoch 8000/10000 Cost: 1.298627\n",
            "Epoch 8100/10000 Cost: 1.298378\n",
            "Epoch 8200/10000 Cost: 1.298135\n",
            "Epoch 8300/10000 Cost: 1.297898\n",
            "Epoch 8400/10000 Cost: 1.297666\n",
            "Epoch 8500/10000 Cost: 1.297438\n",
            "Epoch 8600/10000 Cost: 1.297214\n",
            "Epoch 8700/10000 Cost: 1.296993\n",
            "Epoch 8800/10000 Cost: 1.296774\n",
            "Epoch 8900/10000 Cost: 1.296558\n",
            "Epoch 9000/10000 Cost: 1.296342\n",
            "Epoch 9100/10000 Cost: 1.296127\n",
            "Epoch 9200/10000 Cost: 1.295912\n",
            "Epoch 9300/10000 Cost: 1.295697\n",
            "Epoch 9400/10000 Cost: 1.295481\n",
            "Epoch 9500/10000 Cost: 1.295265\n",
            "Epoch 9600/10000 Cost: 1.295047\n",
            "Epoch 9700/10000 Cost: 1.294827\n",
            "Epoch 9800/10000 Cost: 1.294606\n",
            "Epoch 9900/10000 Cost: 1.294384\n",
            "Epoch 10000/10000 Cost: 1.294161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJu2KdIWA3pH",
        "colab_type": "code",
        "outputId": "735de805-4423-40a8-fbe5-1bc91b3e330d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#정답예측\n",
        "\n",
        "hypothesis=F.softmax(x_train.matmul(W)+b,dim=1)\n",
        "predict=torch.argmax(hypothesis,dim=1)\n",
        "\n",
        "#print(hypothesis)\n",
        "#print(predict)\n",
        "#print(y_train)\n",
        "\n",
        "#정확도 계산\n",
        "correct_prediction=predict.float()==y_train\n",
        "#print(correct_prediction)\n",
        "accuracy=correct_prediction.sum().item()/len(correct_prediction)\n",
        "\n",
        "\n",
        "print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has an accuracy of 87.13% for the training set.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}